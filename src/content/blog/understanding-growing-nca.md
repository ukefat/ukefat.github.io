---
title: 'Recreating Growing Neural Cellular Automata'
pubDate: 2026-02-04
description: 'Recreating the article Growing NCA in PyTorch'
editDate: 2026-02-04
tags: [ml,ca]
heroImage: '../../../public/assets/recreating-growing-nca/partial-smile.png'
---

I first found out about Neural Cellular Automata through this [article by
Distill](https://distill.pub/2020/growing-ca/). It it an excellent article so
you should definitely go read it first. In fact, this post is mostly just an
attempt to recreate their results but with PyTorch instead of TensorFlow. I'm
making this article mostly for my own sake, but read on if you want a shorter
summary and a little bit of my own perspective.

## What are neural cellular automata?

In case you don‚Äôt know, cellular automata are a model of computation where we
typically have a regular 2D grid of cells and where each cell has a specific
state, like on or off. On top of that, we have some rules for how the states
change, such as if my neighbor is on then I turn on too. A simple ruleset can
make some super interesting and complex patters like Conway's game of life
below.

![Conways game of life](../../assets/recreating-growing-nca/Gospers_glider_gun.gif)

Or Stephen Wolfram's Rule 30.

![Rule30](../../assets/recreating-growing-nca/Rule30-256-rows.png)

Neural cellular automata are the exact same concept, but instead of having
specific programmed rules like above, we can treat each cell like a little deep
learning model that we can train. Each cell has the exact same trained weights
and they all perform the same calculations, although their states will be
different because the current internal state and state of the cell‚Äôs neighbors
will influence the output of this calculation.

Neural cellular automata are interesting because now we don‚Äôt have to predefine
the rules, but instead have some specific behavior in mind and update the
weights so that we have some update rules that would otherwise be incredibly
difficult to figure out analytically. For example, training all the little
cells to work together to make a lizard or happy face despite only knowing
about their current surroundings. That‚Äôs what we are going to do now!

## Recreating growing neural cellular automata

I am going to try to be brief here because you can just read the incredible
article itself, which even has a fun interactive demo to go with it. The goal
of the model is to feed in a single seed, 1x1 cell, and have it grow into our
target image, such as a smiley face.

![growing smiley face](../../assets/recreating-growing-nca/growing-smiley.png)

In essence, each cell has 16 states, 4 for red, green, blue, and alpha which is
the aspect that we see and calculate our loss with compared to the target
image. The remaining 12 states are hidden and left up to the cells to decide
how to use.

Every cycle, each cell senses only the 3x3 cells around it (as well as itself).
In the article, this entails two sobel filters and an identity matrix. For my
model, I added an additional laplace filter. These filters are just like
kernels as in a convolutional neural network. So for each cycle, each cell
senses using 4 matrices for each of the 16 channels, resulting in 64 total
inputs to each cell.
```python
def perception(x):
    device = x.device

    identity = torch.tensor([[0,0,0],[0,1,0],[0,0,0]], dtype=torch.float32)
    sobel_x  = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32)/8.0
    sobel_y  = sobel_x.T
    laplace  = torch.tensor([[0,1,0],[1,-4,1],[0,1,0]], dtype=torch.float32)/8.0

    kernels = torch.stack([identity, sobel_x, sobel_y, laplace])
    kernels = kernels[:, None, :, :].to(device)

    kernels = kernels.repeat(C, 1, 1, 1)

    return F.conv2d(x, kernels, padding=1, groups=C)
```
In addition, we only sense the surrounding cells if they are alive. If the cell
has alpha of 0.2 or more, then it is alive. Only alive cells can do anything
and if they are alive, they cause the cells around them to be ‚Äúgrowing‚Äù. The
‚Äúgrowing cells‚Äù don‚Äôt cause the cells beside them to be alive.

Now, the actual trained parameters are just a feed-forward neural network with
a 128-neuron hidden layer. Therefore, each cell takes in 64 inputs, transforms
to 128-neuron hidden state, and then finally needs to output its new state,
which is 16 channels. This leads to 64 * 128 + 128 * 16 = 10240 trainable
parameters.

<img src="../../../public/assets/recreating-growing-nca/model-diagram-updated.png" class="img-large"/>

Here is our code for the model. It is surprisingly simple given the complex task.

```python
class NCA(nn.Module):
    def __init__(self, channels=C, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(channels * 4, hidden, 1),
            nn.ReLU(),
            nn.Conv2d(hidden, channels, 1, bias=False)
        )

        nn.init.zeros_(self.net[-1].weight)

    def forward(self, x, fire_rate=0.5):
        y = perception(x)
        dx = self.net(y)

        # stochastic update mask
        update_mask = (torch.rand_like(x[:, :1]) <= fire_rate).float()
        x = x + dx * update_mask
        x = x * alive_mask(x)
        return x
```
*Note: we could have made the filters/kernels trainable parameters to allow the
model to decide what to sense, but the original article just used fixed
filters, so I‚Äôm following same thing. Also makes things simpler!*

### Training the model

Now, to train the model, we just start with a single seed and let it update
around 70 steps. After that, we see calculate the loss, how close it was to the
target image, then we backpropagate through time as in a RNN. We update the
weights accordingly and continue this process until our model successfully
grows from seed to target image. We can see the output below!

<img src="../../../public/assets/recreating-growing-nca/growing-microbe.gif" class="pixelated"/>

This works surprisingly well, except the problem is that we only trained our
model to *just reach* the target state and not maintain it. Therefore, in most
cases, after we reach our 70 steps, our model has never been trained to go
beyond this point, so it doesn't know what to do after reaching the target, and
it just falls apart...

<img src="../../../public/assets/recreating-growing-nca/microbe_collapse.gif" class="pixelated"/>

### Maintaing form with pooling

In order to get our model to keep its form, we need to train it to start off
with more or less the target image and keep it there. We do this by using a
pool of starting samples instead of just always starting from a single seed.

First, we create a pool full of just single seeds. We pipe those samples into
the model and then we take whatever the model outputted and throw it into the
pool. Now the model has a chance to grab a sample from the pool that is at
least a little more than a single seed. As the model gets better, the pool will
mostly just be full of samples very close to the target image, so the model now
has the goal of maintaining that target image for about 70 samples.

We also make sure to always ensure that at least one sample is the batch is
just a single seed so that we never forget how to grow from a single seed. 

Look at how the pool slowly gets full of states closer to the target image.

![pool_0000](../../assets/recreating-growing-nca/0000_pool.jpg)
![pool_0300](../../assets/recreating-growing-nca/0300_pool.jpg)
![pool_0700](../../assets/recreating-growing-nca/0700_pool.jpg)
![pool_2000](../../assets/recreating-growing-nca/2000_pool.jpg)

With that implemented, we retrain our model and see that it does a much better
job of maintaining itself.

<img src="../../../public/assets/recreating-growing-nca/long_microbe_maintain.gif" class="pixelated"/>

### Regenerating

The article goes one step further and tests to see if the model can regenerate
itself if part of it is damaged, similar to a reptile that regrow a tail when
it is cut off.

![piccolo regrow arm](https://media1.tenor.com/m/8hh4MkOadoIAAAAC/piccolo-regeneration.gif)

We can train our model to do this by just repeatedly punchings holes in our
samples.

![goten punching throwing 1000 punches](https://media1.tenor.com/m/4GMfoIq0xaIAAAAC/goten-rapid-fire-fist.gif)

We make holes in some of the input samples so that the model is forced to learn
how to regrow to the target from various damaged states. As you can see below,
we just damage a few of the samples, in this case, just the last 3 of the
batch.

![damaged inputs](../../assets/recreating-growing-nca/iter_3400_input.png)

This works quite well. Even when slicing it in half which we didn‚Äôt train on,
the cells are all still able to work together to return to the target state.

<img src="../../../public/assets/recreating-growing-nca/nca_regen.gif" class="pixelated"/>

Also note, the microbe is a lot less detailed than before. It is missing the
noodles that were previously sticking out of it. After implementing damage, the
model struggled to learn to be both be able to regenerate from different
damaged positions as well as create a very accuracte reproduction of the target
image. I assume if I added more parameters and training time this could be
resolved, but I didn't want to wait any longer for training models üòõ. 

## What did I learn?

Since this post is mostly for my own sake, I'm just going to go over the big things that I've learned from working on this project.

### Building cellular automata is easy to get into

I've never messed with any cellular automata stuff before this and I was
surprised at how easy it was. I was worried about how to render the pixels and
manage the state and things, but in reality you can just make a big numpy
array, play with it as you want, and then display with matplotlib using a
single line of code.

In addition, despite there not being any specific built-in functions in PyTorch
for NCA, everything was very straightforward. I was able to just use the
functions typically used for CNN's and RNN's to do convolutions and backprop
through time with ease. I just had to treat all of the cells as a 1x1
convolution, so I got a system that was simple as well as performant thanks to
being able to use all of PyTorch's optimizations. Training a model never took
much more than 5-10 minutes on Google Colab.

### More "complex" images can be easier to learn

I mentioned briefly earlier that I struggled to get the microbe image to
regenerate as well as accurately reproduce the target image. I spent a good
amount of time trying to fix this problem. I had briefly tried using more
layers in the feed-forward neural network, but it didn't seem to fix my problem
much. I also found a few small errors in my code that might have been
contributing to the issue.

I eventually later tried the exact same model that was failing to accurately
repoduce the microbe emoji, except on the lizard emoji.

<img src="../../../public/assets/recreating-growing-nca/nca_pwned_liz.gif"
class="pixelated"/>

First of all, the lizard already seems more accurate to the target image,
everything is quite faithful, at least with the resolution down a good amount.
In addition, even after slicing it in half and blowing a big hole in it, the
lizard regenerates really well. The lizard seems so much more complex than the
simple microbe which is just a blob, but the NCA seems to perform much better
with the lizard. What's going on? Maybe lizards look the way they do due to
some particular structure about them, which makes it easier to regenerate,
which is why they look how they do.

My guess is that the NCA are just better at learning local structure, and with
the lizard this is more apparent. There is centre spot in the body which is
more yellow and has some dotted pattern on the back. If you just took a small
5x5 snippet of the image, I think you'd have a better chance of knowing where
you are compared to the microbe. With a microbe, there is just a lot of green
blob without much variation between edges. This may make it harder to learn.

This is just a guess and I'm sure there are some interesting experiments I
could do in the future to better validate this by trying to grow different
images with different levels of complexity or local structure.

In fact, I was trying to design some of my own lizards and trying out different
ways of encoding structure into the image itself to hopefully help the NCA to
learn. I thought of things like colour-coding repeating structures or building
using more circles and repeating structure, but I ultimately decided to
postpone this for some other time. Here are some of the lizards I drew though.

<img src="../../../public/assets/recreating-growing-nca/sexy-back.png"
class="pixelated img-small"/>
<img src="../../../public/assets/recreating-growing-nca/colour-lizard.png"
class="pixelated img-small"/>
<img src="../../../public/assets/recreating-growing-nca/frog.png"
class="pixelated img-small"/>

### Normalizing is important

Maybe this is just me, but normalization never feels that important whenever I
am reading about deep learning. It always seems just tacked on the end as
something that might slightly increase performance, and for that reason, I
typically forget about it. However, in this project I definitely realized more
why we need normalization.

One of the big things that helped me improve the model to help generate the
microbe better was normalizing gradients and normalizing convolutions. I guess
I missed this when implementing my model because it is subtle and not mentioned
except in the footnotes of the original article, although it is definitely
there in their example code.

Normalization makes a lot of sense here because for each iteration we are
recursively updating the output up to almost 100 steps. That means we are
backpropagating through time 100 steps, so if there are gradients or
convolutions above 1, then gradients will explode. I experienced this with lots
of fluctuations in the loss while training and after normalization, training
always seemed qualitatively smoother and I was able to better reduce loss.

## Conclusion

NCA are very cool and I plan on doing some more experimentation with this
system in the future. My original idea was maybe to try and find out a little
bit about what the NCA are actually doing when the build the target image, but
I found I already had a good amount here to display just from recreating the
system from the article. Hopefully I can do some more cool stuff soon!
